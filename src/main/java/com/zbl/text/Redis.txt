【redis普通分布式锁存在一定的缺陷】
    [高可用问题]
    客户端1在Redis的master节点上拿到了锁
    Master宕机了，存储锁的key还没有来得及同步到Slave上
    master故障，发生故障转移，slave节点升级为master节点
    客户端2从新的Master获取到了对应同一个资源的锁
    　　于是，客户端1和客户端2同时持有了同一个资源的锁。锁的安全性被打破了。针对这个问题。
        Redis作者antirez提出了RedLock算法来解决这个问题

    [业务线超时问题]
    节点1：如果设置锁的过期时间为30MS,但是业务线可能因为网络或者数据量峰值出现导致执行时间超过了30MS，
           那么这时redis已经把锁给释放了，但是业务线却仍然在执行
    节点2 这时去获取锁，发现锁可以获取成功，这就造成了 同时有两个节点在执行同一个业务逻辑，
          则无法保证业务的幂等性（数据加上版本号处理，但仍然会对累加结果造成重复性错误），
          会造成数据重复处理，或者日志主键ID重复，同一订单两次计算金额，客户两次扣款等问题

    [Redisson]
    Redisson在这个问题上加上了自动续时，如果锁已经被持有，那么另一个线程试图再次获取锁时，会把已存在的锁重置过期时间，
    就相当于延长了当前锁的时间从而避免过期造成幂等性问题(采用LUA脚本代码加锁，LUA脚本在redis中具有原子性操作。)
    watch dog自动延期机制
    客户端1加锁的锁key默认生存时间才30秒，如果业务执行时间超过了30秒，客户端1还是需要一直持有这把锁，怎么办呢？
    简单！只要客户端1一旦加锁成功，就会启动一个watch dog看门狗，他是一个后台线程，会每隔10秒检查一下，
    如果客户端1还持有锁key，那么就会不断的延长锁key的生存时间。

    [Redisson分布式锁的缺点]
    其实上面那种方案最大的问题，就是如果你对某个redis master实例，写入了myLock这种锁key的value，此时会异步复制给对应的master slave实例。
    但是这个过程中一旦发生redis master宕机，主备切换，redis slave变为了redis master。
    接着就会导致，客户端2来尝试加锁的时候，在新的redis master上完成了加锁，而客户端1也以为自己成功加了锁。
    此时就会导致多个客户端对一个分布式锁完成了加锁。
    这时系统在业务语义上一定会出现问题，导致各种脏数据的产生。
    所以这个就是redis cluster，或者是redis master-slave架构的主从异步复制导致的redis分布式锁的最大缺陷：
    在redis master实例宕机的时候，可能导致多个客户端同时完成加锁，
    如果要追求强一致性，那么只能考虑zookeeper分布式锁，当然它们各有自己的优缺点。

